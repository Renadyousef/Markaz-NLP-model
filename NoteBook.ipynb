{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c887ed1b-00ef-4d27-8c0c-9fa8226148cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "import torch\n",
    "device = 0 if torch.cuda.is_available() else -1  # 0 = first GPU, -1 = CPU for speed\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from arabert.preprocess import ArabertPreprocessor #preprocess text\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "load_dotenv()  # Loads HF_TOKEN automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0084b9be-5db8-4562-8dbf-987af6d5271d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\activations_tf.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 2\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtoken-classification\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCAMeL-Lab/bert-base-arabic-camelbert-da-pos-msa\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mإمارة أبوظبي هي إحدى إمارات دولة الإمارات العربية المتحدة السبع\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m pos(text)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\__init__.py:1028\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_name, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1027\u001b[0m     feature_extractor \u001b[38;5;241m=\u001b[39m model_name\n\u001b[1;32m-> 1028\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1029\u001b[0m     feature_extractor \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1031\u001b[0m     \u001b[38;5;66;03m# Impossible to guess what is the right feature_extractor here\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:268\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m all_traceback \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_class \u001b[38;5;129;01min\u001b[39;00m class_tuple:\n\u001b[1;32m--> 268\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m model\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    270\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:2302\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:2332\u001b[0m, in \u001b[0;36m_get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:2330\u001b[0m, in \u001b[0;36m_get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     30\u001b[0m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     31\u001b[0m     TFBaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     TFTokenClassifierOutput,\n\u001b[0;32m     39\u001b[0m )\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     41\u001b[0m     TFCausalLanguageModelingLoss,\n\u001b[0;32m     42\u001b[0m     TFMaskedLanguageModelingLoss,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m     unpack_inputs,\n\u001b[0;32m     54\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\activations_tf.py:27\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m         )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_gelu\u001b[39m(x):\n\u001b[0;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    initially created. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) Also see\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    https://arxiv.org/abs/1606.08415\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pos = pipeline('token-classification', model='CAMeL-Lab/bert-base-arabic-camelbert-da-pos-msa')\n",
    "text = 'إمارة أبوظبي هي إحدى إمارات دولة الإمارات العربية المتحدة السبع'\n",
    "pos(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c937f974-64ae-4a44-8463-cfbacbb81c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(extracted_text: str):\n",
    "    \"\"\"\n",
    "    Preprocess text using AraBERT preprocessor.\n",
    "    Returns cleaned text only for Formal arabic text only.\n",
    "    \"\"\"\n",
    "    cleaned_text = arabert_prep.preprocess(extracted_text)\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67077342-b8d8-464f-b181-02f49502273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_quiz(processed_data, quiz_level=\"easy\"):\n",
    "    \"\"\"\n",
    "    Generate quiz questions (True/False + MCQ) based on difficulty level.\n",
    "    \"\"\"\n",
    "    entities = processed_data[\"entities\"]\n",
    "    questions = []\n",
    "\n",
    "    # Example: build simple True/False questions from entities\n",
    "    for e in entities[:5]:  # limit for demo\n",
    "        q = {\n",
    "            \"question\": f\"الكلمة '{e['word']}' هي {e['entity']}؟\",\n",
    "            \"type\": \"TF\",\n",
    "            \"correct_answer\": \"صح\"\n",
    "        }\n",
    "        questions.append(q)\n",
    "\n",
    "    # TODO: Add MCQ generation and difficulty adjustment\n",
    "    if quiz_level == \"medium\":\n",
    "        pass\n",
    "    elif quiz_level == \"hard\":\n",
    "        pass\n",
    "\n",
    "    return questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fbe93dd1-088d-4231-b084-ac4b76df7758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\renad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'farasa-api.qcri.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 241M/241M [01:08<00:00, 3.51MiB/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-17 16:47:13,769 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize AraBERT preprocessor & tokenizer\n",
    "arabert_model_name = \"aubmindlab/bert-base-arabertv2\"\n",
    "arabert_prep = ArabertPreprocessor(model_name=arabert_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(arabert_model_name)\n",
    "\n",
    "def make_flashCards(cleaned_text: str, max_flashcards: int = 10):\n",
    "    \"\"\"\n",
    "    Generate flashcards using AraBERT tokenizer.\n",
    "    Each token (heuristically considered a noun) becomes a term.\n",
    "    Definition = sentence containing the token.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(cleaned_text)\n",
    "    flashcards = []\n",
    "    added_terms = set()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Preprocess & tokenize sentence\n",
    "        cleaned_sentence = arabert_prep.preprocess(sentence)\n",
    "        tokens = tokenizer.tokenize(cleaned_sentence)\n",
    "\n",
    "        # Simple heuristic: tokens longer than 2 chars and not already added\n",
    "        for token in tokens:\n",
    "            if len(token) > 2 and token not in added_terms:\n",
    "                flashcards.append({\n",
    "                    \"term\": token,\n",
    "                    \"definition\": sentence.strip()\n",
    "                })\n",
    "                added_terms.add(token)\n",
    "\n",
    "            if len(flashcards) >= max_flashcards:\n",
    "                break\n",
    "        if len(flashcards) >= max_flashcards:\n",
    "            break\n",
    "\n",
    "    return flashcards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92f09568-e0e3-4af1-8ea1-1ede530b8a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Flashcards ---\n",
      "{'term': 'محمد', 'definition': 'محمد هو B-PER'}\n",
      "{'term': 'علي', 'definition': 'علي هو I-PER'}\n",
      "{'term': 'القاهرة', 'definition': 'القاهرة هو B-LOC'}\n",
      "{'term': 'مايكروسوفت', 'definition': 'مايكروسوفت هو B-ORG'}\n",
      "\n",
      "--- Quiz ---\n",
      "{'question': \"الكلمة 'محمد' هي B-PER؟\", 'type': 'TF', 'correct_answer': 'صح'}\n",
      "{'question': \"الكلمة 'علي' هي I-PER؟\", 'type': 'TF', 'correct_answer': 'صح'}\n",
      "{'question': \"الكلمة 'القاهرة' هي B-LOC؟\", 'type': 'TF', 'correct_answer': 'صح'}\n",
      "{'question': \"الكلمة 'مايكروسوفت' هي B-ORG؟\", 'type': 'TF', 'correct_answer': 'صح'}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    text = \"ولد محمد علي في القاهرة وعمل في شركة مايكروسوفت.\"\n",
    "    processed = generator(text)\n",
    "\n",
    "    print(\"\\n--- Flashcards ---\")\n",
    "    for f in make_flashCards(processed):\n",
    "        print(f)\n",
    "\n",
    "    print(\"\\n--- Quiz ---\")\n",
    "    for q in make_quiz(processed, quiz_level=\"easy\"):\n",
    "        print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "015d24f9-f69f-427d-868c-899b3c97fd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Flashcards ---\n",
      "Term: خوارزمي\n",
      "Definition: ال+ خوارزمي +ة هي مجموع +ة من ال+ خطو +ات ال+ محدد +ة ل+ حل مشكل +ة أو تنفيذ مهم +ة .\n",
      "--------------------\n",
      "Term: مجموع\n",
      "Definition: ال+ خوارزمي +ة هي مجموع +ة من ال+ خطو +ات ال+ محدد +ة ل+ حل مشكل +ة أو تنفيذ مهم +ة .\n",
      "--------------------\n",
      "Term: خطو\n",
      "Definition: ال+ خوارزمي +ة هي مجموع +ة من ال+ خطو +ات ال+ محدد +ة ل+ حل مشكل +ة أو تنفيذ مهم +ة .\n",
      "--------------------\n",
      "Term: محدد\n",
      "Definition: ال+ خوارزمي +ة هي مجموع +ة من ال+ خطو +ات ال+ محدد +ة ل+ حل مشكل +ة أو تنفيذ مهم +ة .\n",
      "--------------------\n",
      "Term: مشكل\n",
      "Definition: ال+ خوارزمي +ة هي مجموع +ة من ال+ خطو +ات ال+ محدد +ة ل+ حل مشكل +ة أو تنفيذ مهم +ة .\n",
      "--------------------\n",
      "Term: تنفيذ\n",
      "Definition: ال+ خوارزمي +ة هي مجموع +ة من ال+ خطو +ات ال+ محدد +ة ل+ حل مشكل +ة أو تنفيذ مهم +ة .\n",
      "--------------------\n",
      "Term: مهم\n",
      "Definition: ال+ خوارزمي +ة هي مجموع +ة من ال+ خطو +ات ال+ محدد +ة ل+ حل مشكل +ة أو تنفيذ مهم +ة .\n",
      "--------------------\n",
      "Term: ولد\n",
      "Definition: ولد محمد علي في ال+ قاهر +ة و+ عمل في شرك +ة مايكروسوفت .\n",
      "--------------------\n",
      "Term: محمد\n",
      "Definition: ولد محمد علي في ال+ قاهر +ة و+ عمل في شرك +ة مايكروسوفت .\n",
      "--------------------\n",
      "Term: علي\n",
      "Definition: ولد محمد علي في ال+ قاهر +ة و+ عمل في شرك +ة مايكروسوفت .\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"\"\"\n",
    "الخوارزمية هي مجموعة من الخطوات المحددة لحل مشكلة أو تنفيذ مهمة.\n",
    "ولد محمد علي في القاهرة وعمل في شركة مايكروسوفت.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Preprocess text\n",
    "cleaned_text = pre_process(text)\n",
    "\n",
    "# Step 2: Generate flashcards\n",
    "flashcards = make_flashCards(cleaned_text)\n",
    "\n",
    "# Step 3: Print results\n",
    "print(\"--- Flashcards ---\")\n",
    "for f in flashcards:\n",
    "    print(f\"Term: {f['term']}\")\n",
    "    print(f\"Definition: {f['definition']}\")\n",
    "    print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8dc1df06-defc-44e3-b223-12a05523ecef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Some weights of the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-da-pos-msa were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER results: [{'entity_group': 'PERS', 'score': 0.981635, 'word': 'محمد علي', 'start': 4, 'end': 12}, {'entity_group': 'LOC', 'score': 0.9977779, 'word': 'القاهرة', 'start': 16, 'end': 23}, {'entity_group': 'ORG', 'score': 0.9847733, 'word': 'مايكروسوفت', 'start': 37, 'end': 47}]\n",
      "POS results: [{'entity': 'verb', 'score': 0.99997437, 'index': 1, 'word': 'ولد', 'start': 0, 'end': 3}, {'entity': 'noun_prop', 'score': 0.99997926, 'index': 2, 'word': 'محمد', 'start': 4, 'end': 8}, {'entity': 'noun_prop', 'score': 0.99996734, 'index': 3, 'word': 'علي', 'start': 9, 'end': 12}, {'entity': 'prep', 'score': 0.999979, 'index': 4, 'word': 'في', 'start': 13, 'end': 15}, {'entity': 'noun_prop', 'score': 0.99998593, 'index': 5, 'word': 'القاهرة', 'start': 16, 'end': 23}, {'entity': 'verb', 'score': 0.9999715, 'index': 6, 'word': 'وعمل', 'start': 24, 'end': 28}, {'entity': 'prep', 'score': 0.99998415, 'index': 7, 'word': 'في', 'start': 29, 'end': 31}, {'entity': 'noun', 'score': 0.9999869, 'index': 8, 'word': 'شركة', 'start': 32, 'end': 36}, {'entity': 'noun_prop', 'score': 0.99997413, 'index': 9, 'word': 'مايكروسوفت', 'start': 37, 'end': 47}, {'entity': 'punc', 'score': 0.9999927, 'index': 10, 'word': '.', 'start': 47, 'end': 48}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# NER model\n",
    "ner_model_name = \"CAMeL-Lab/bert-base-arabic-camelbert-mix-ner\"\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(ner_model_name)\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(ner_model_name)\n",
    "ner_pipeline = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# POS model\n",
    "pos_model_name = \"CAMeL-Lab/bert-base-arabic-camelbert-da-pos-msa\"\n",
    "pos_tokenizer = AutoTokenizer.from_pretrained(pos_model_name)\n",
    "pos_model = AutoModelForTokenClassification.from_pretrained(pos_model_name)\n",
    "pos_pipeline = pipeline(\"token-classification\", model=pos_model, tokenizer=pos_tokenizer, aggregation_strategy=\"none\")\n",
    "\n",
    "# Example text\n",
    "text = \"ولد محمد علي في القاهرة وعمل في شركة مايكروسوفت.\"\n",
    "\n",
    "# NER\n",
    "ner_results = ner_pipeline(text)\n",
    "print(\"NER results:\", ner_results)\n",
    "\n",
    "# POS\n",
    "pos_results = pos_pipeline(text)\n",
    "print(\"POS results:\", pos_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c064f56e-4034-461d-b3cc-192acc5c6764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
